{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import argparse\n",
    "import models.models.DnCNN as DnCNN, models.models.ResNet as ResNet, models.models.RFDN as RFDN\n",
    "import models.models.DRLN as DRLN, models.models.pix2pix as pix2pix, models.models.pix2pix2 as pix2pix2\n",
    "from models.models.network_swinir import SwinIR as net\n",
    "from models.utils import util_calculate_psnr_ssim as util\n",
    "import models.utils.vgg_loss, models.utils.vgg_perceptual_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from models.utils.param import seed_everything\n",
    "\n",
    "model_list = ['DnCNN', 'ResNet18', 'RFDN', 'DRLN', 'pix2pix', 'pix2pix2', 'swinir']\n",
    "\n",
    "sel_model = model_list[6]\n",
    "pth_num =6\n",
    "noise = False\n",
    "val = 0.001\n",
    "show_after_cv = False\n",
    "clean_nrom = False\n",
    "if noise: pth = '.pth'\n",
    "else: pth = '_clean.pth' \n",
    "if sel_model=='pix2pix' or sel_model=='pix2pix2': pth='_g.pth'\n",
    "save_record = False # 기록용 폴더에 있으면\n",
    "pth_dir ='./save/best_'+sel_model+'_model'+str(pth_num)+pth\n",
    "if save_record:\n",
    "    pth_dir ='./기록용/'+sel_model+'/'+'best_'+sel_model+'_model'+str(pth_num)+pth\n",
    "# load_loss_pth ='./ResNet18_model2.pth'\n",
    " # SwinIR.init.pth\n",
    "# pth_dir = './save/SwinIR.init.pth'\n",
    "\n",
    "display_number = 20                           # plt로 디스플레이할 이미지의 개수\n",
    "residual_output_paths = '/content/residuals'           # 잔차 이미지를 저장할 폴더\n",
    "noisy_image_paths = '/local_datasets/MLinP/train/scan'\n",
    "clean_image_paths = '/local_datasets/MLinP/train/clean'\n",
    "\n",
    "seed_everything(42)\n",
    "output = False                                         # residual을 파일로 출력할지 여부\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def cvt_to_image(image, norm=False):\n",
    "    image = image.cpu().squeeze(0)\n",
    "    image = torch.clamp(image, 0, 1)  # 이미지 값을 0과 1 사이로 클램핑\n",
    "    if norm:\n",
    "        image = ToPILImage()(image*0.5+0.5)\n",
    "    else:\n",
    "        image = ToPILImage()(image)\n",
    "    return image\n",
    "\n",
    "class CustomDatasetTest(data.Dataset):\n",
    "    def __init__(self, noisy_image_paths, clean_image_paths, transform=None, transform_clean=None):\n",
    "        self.noisy_image_paths = [join(noisy_image_paths, x) for x in listdir(noisy_image_paths)]\n",
    "        self.clean_image_paths = [join(clean_image_paths, x) for x in listdir(clean_image_paths)]\n",
    "        self.transform = transform\n",
    "        self.transform_clean = transform_clean\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        noisy_image_path = self.noisy_image_paths[index]\n",
    "        noisy_image = load_img(self.noisy_image_paths[index])\n",
    "        clean_image_path = self.clean_image_paths[index]\n",
    "        clean_image = load_img(self.clean_image_paths[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "            clean_image = self.transform_clean(clean_image)\n",
    "\n",
    "        return noisy_image, noisy_image_path, clean_image, clean_image_path\n",
    "\n",
    "def tensor_to_yuv(images):\n",
    "    i =[]\n",
    "    for image in images:\n",
    "        image = torch.clamp(image, 0, 1)  # 이미지 값을 0과 1 사이로 클램핑\n",
    "        image = ToPILImage()(image)\n",
    "        image = np.array(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "        i.append(image)\n",
    "    return i\n",
    "\n",
    "def cal_mae_loss(ps,ts, y=False):\n",
    "    abss = 0.0\n",
    "    for p, t in zip(ps,ts):\n",
    "        if not y:\n",
    "            p = p[:, :, 0]\n",
    "            t = t[:, :, 0]\n",
    "        abss += abs(np.mean(p.flatten()) - np.mean(t.flatten()))\n",
    "    return abss\n",
    "\n",
    "def BilateralBlur(image):\n",
    "    image = image.squeeze(0)\n",
    "    image = torch.clamp(image,0,1)\n",
    "    image = ToPILImage()(image)\n",
    "    image = np.array(image)\n",
    "    image = cv2.bilateralFilter(image, -1, 50,50)\n",
    "    image = ToTensor()(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "    \n",
    "\n",
    "if sel_model == 'DnCNN':\n",
    "    model = DnCNN.DnCNN()\n",
    "elif sel_model == 'ResNet18':\n",
    "    model = ResNet.ResNet18()\n",
    "elif sel_model == 'RFDN':\n",
    "    model = RFDN.RFDN()\n",
    "elif sel_model == 'DRLN':\n",
    "    model = DRLN.DRLN()\n",
    "elif sel_model == 'pix2pix':\n",
    "    model = pix2pix.Generator()\n",
    "elif sel_model == 'pix2pix2':\n",
    "    model = pix2pix2.GeneratorUNet()\n",
    "    model_D = pix2pix2.Discriminator()\n",
    "elif sel_model == 'swinir':\n",
    "    model = net(upscale=1, in_chans=3, img_size=128, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "model.load_state_dict(torch.load(pth_dir, map_location='cpu'))\n",
    "model.eval()\n",
    "# GPU 사용 여부 확인\n",
    "model.to(device)\n",
    "\n",
    "test_transform = Compose([\n",
    "    # BilateralBlur(512),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "if clean_nrom:\n",
    "    test_clean_transform = Compose([\n",
    "        # BilateralBlur(512),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "else:\n",
    "    test_clean_transform = Compose([\n",
    "        # BilateralBlur(512),\n",
    "        ToTensor()\n",
    "    ])\n",
    "# 데이터셋 로드 및 전처리\n",
    "dataset = CustomDatasetTest(noisy_image_paths, clean_image_paths ,transform=test_transform,transform_clean=test_clean_transform)\n",
    "# val 분할\n",
    "train_size = int(len(dataset)*(1-val))\n",
    "val_size = int(len(dataset)*(val))\n",
    "train_dataset, val_dataset = random_split(dataset,[train_size,val_size])\n",
    "# 데이터 로더 설정\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "tot_mae = 0.0\n",
    "tot_after_mae = 0.0\n",
    "mae_after_loss = 0.0\n",
    "print('하는중')\n",
    "with torch.no_grad():\n",
    "    for noisy_image, _, clean_image, _  in val_loader:\n",
    "        # noisy_image = BilateralBlur(noisy_image)\n",
    "        noisy_image = noisy_image.to(device)\n",
    "        if noise:\n",
    "            pred_noise = model(noisy_image)\n",
    "            denoised_image = noisy_image - pred_noise  \n",
    "        else:\n",
    "            denoised_image = model(noisy_image) #.tolist()\n",
    "            if show_after_cv:\n",
    "                denoised_after_image = BilateralBlur(denoised_image)\n",
    "\n",
    "            pred_noise = (noisy_image.cpu()*0.5+0.5) - denoised_image.cpu()\n",
    "        # denoised_image를 CPU로 이동하여 이미지 저장\n",
    "        residual_image = (noisy_image.cpu()*0.5+0.5) - clean_image\n",
    "\n",
    "        mae_loss = cal_mae_loss(tensor_to_yuv(denoised_image),tensor_to_yuv(clean_image))\n",
    "        tot_mae += mae_loss\n",
    "        if show_after_cv:\n",
    "            mae_after_loss = cal_mae_loss(tensor_to_yuv(denoised_after_image),tensor_to_yuv(clean_image))\n",
    "            tot_after_mae += mae_after_loss\n",
    "            denoised_after_image = cvt_to_image(denoised_after_image, clean_nrom)\n",
    "        \n",
    "        clean_image = cvt_to_image(clean_image, clean_nrom)\n",
    "        denoised_image = cvt_to_image(denoised_image, clean_nrom)\n",
    "        noisy_image = cvt_to_image(noisy_image,norm=True)\n",
    "        residual_image = cvt_to_image(residual_image, norm=False)\n",
    "        pred_noise = cvt_to_image(pred_noise, norm=False)\n",
    "        print('This MAE: ',mae_loss)\n",
    "        print('This after MAE: ',mae_after_loss)\n",
    "        if True:\n",
    "            print(_)\n",
    "            plt.figure(figsize=(20,13))\n",
    "            plt.subplot(2,3,1)\n",
    "            plt.imshow(noisy_image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(2,3,2)\n",
    "            plt.imshow(clean_image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(2,3,3)\n",
    "            plt.imshow(denoised_image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(2,3,4)\n",
    "            plt.imshow(residual_image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(2,3,5)\n",
    "            plt.imshow(pred_noise, cmap='gray')\n",
    "            plt.axis('off')\n",
    "            if show_after_cv:\n",
    "                plt.subplot(2,3,6)\n",
    "                plt.imshow(denoised_after_image, cmap='gray')\n",
    "                plt.axis('off')\n",
    "            plt.show()\n",
    "print(len(val_loader),'개의', 'total MAE: ', tot_mae/len(val_loader))\n",
    "print(len(val_loader),'개의', 'total after MAE: ', tot_after_mae/len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This MAE:  53.42594909667969\n",
      "This MAE:  55.935001373291016\n",
      "This MAE:  21.430633544921875\n",
      "This MAE:  6.248546600341797\n",
      "This MAE:  41.061161041259766\n",
      "This MAE:  86.50758743286133\n",
      "This MAE:  47.986595153808594\n",
      "This MAE:  29.028141021728516\n",
      "This MAE:  83.20027160644531\n",
      "This MAE:  12.48593521118164\n",
      "This MAE:  113.15591812133789\n",
      "This MAE:  96.23281478881836\n",
      "This MAE:  27.004878997802734\n",
      "This MAE:  113.36851501464844\n",
      "This MAE:  48.8288459777832\n",
      "This MAE:  80.69601821899414\n",
      "This MAE:  11.784053802490234\n",
      "This MAE:  15.175090789794922\n",
      "This MAE:  109.66451644897461\n",
      "This MAE:  57.6977653503418\n",
      "This MAE:  103.34006881713867\n",
      "This MAE:  19.74368667602539\n",
      "This MAE:  40.421165466308594\n",
      "This MAE:  102.66105651855469\n",
      "This MAE:  89.98386764526367\n",
      "This MAE:  38.45386505126953\n",
      "This MAE:  26.25112533569336\n",
      "This MAE:  7.453609466552734\n",
      "This MAE:  112.30030059814453\n",
      "This MAE:  42.342201232910156\n",
      "This MAE:  8.397647857666016\n",
      "This MAE:  95.58625411987305\n",
      "This MAE:  52.18144607543945\n",
      "This MAE:  10.93276596069336\n",
      "This MAE:  81.77693176269531\n",
      "This MAE:  56.9710578918457\n",
      "This MAE:  16.204124450683594\n",
      "This MAE:  79.81632995605469\n",
      "This MAE:  104.78126907348633\n",
      "This MAE:  75.8289909362793\n",
      "This MAE:  104.72496032714844\n",
      "This MAE:  105.8798942565918\n",
      "This MAE:  48.474266052246094\n",
      "This MAE:  66.05840301513672\n",
      "This MAE:  68.11724090576172\n",
      "This MAE:  78.14461898803711\n",
      "This MAE:  23.917457580566406\n",
      "This MAE:  11.113555908203125\n",
      "This MAE:  91.56634140014648\n",
      "This MAE:  103.60464477539062\n",
      "This MAE:  75.9676628112793\n",
      "This MAE:  54.883758544921875\n",
      "This MAE:  111.4827766418457\n",
      "This MAE:  22.32526397705078\n",
      "This MAE:  101.69811248779297\n",
      "This MAE:  83.40559768676758\n",
      "This MAE:  44.7833366394043\n",
      "This MAE:  99.5414047241211\n",
      "This MAE:  85.2752799987793\n",
      "This MAE:  31.280590057373047\n",
      "This MAE:  109.11000442504883\n",
      "This MAE:  92.91238403320312\n",
      "This MAE:  80.12446212768555\n",
      "This MAE:  78.42977523803711\n",
      "This MAE:  7.0617218017578125\n",
      "This MAE:  89.1395492553711\n",
      "This MAE:  1.4304885864257812\n",
      "This MAE:  105.72855758666992\n",
      "This MAE:  105.61275863647461\n",
      "This MAE:  32.29306411743164\n",
      "This MAE:  76.59414672851562\n",
      "This MAE:  136.38392639160156\n",
      "This MAE:  41.96571350097656\n",
      "This MAE:  68.82361602783203\n",
      "This MAE:  22.524303436279297\n",
      "This MAE:  80.60787963867188\n",
      "This MAE:  43.99453353881836\n",
      "This MAE:  104.89974594116211\n",
      "This MAE:  76.91348266601562\n",
      "This MAE:  91.93973922729492\n",
      "This MAE:  17.555728912353516\n",
      "This MAE:  100.90031051635742\n",
      "This MAE:  74.73550796508789\n",
      "This MAE:  112.5289306640625\n",
      "This MAE:  91.1369743347168\n",
      "This MAE:  100.25576782226562\n",
      "This MAE:  94.27618789672852\n",
      "This MAE:  91.83629608154297\n",
      "This MAE:  51.98256301879883\n",
      "This MAE:  9.86513900756836\n",
      "This MAE:  21.30721664428711\n",
      "This MAE:  97.30394744873047\n",
      "This MAE:  119.6739501953125\n",
      "This MAE:  36.183040618896484\n",
      "This MAE:  95.11726379394531\n",
      "This MAE:  75.08073043823242\n",
      "This MAE:  39.21846008300781\n",
      "This MAE:  2.1640625\n",
      "This MAE:  16.908184051513672\n",
      "This MAE:  88.56378555297852\n",
      "100 개의 total MAE:  64.71675071716308\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "import torchvision.transforms as tf\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import argparse\n",
    "import models.models.DnCNN as DnCNN, models.models.ResNet as ResNet, models.models.RFDN as RFDN\n",
    "import models.models.DRLN as DRLN, models.models.pix2pix as pix2pix, models.models.pix2pix2 as pix2pix2\n",
    "from models.models.network_swinir import SwinIR as net\n",
    "# from models.utils.param import param_check, seed_everything\n",
    "from models.utils.param import seed_everything\n",
    "import models.utils.vgg_loss, models.utils.vgg_perceptual_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_list = ['DnCNN', 'ResNet18', 'RFDN', 'DRLN', 'pix2pix']\n",
    "\n",
    "sel_model = model_list[4]\n",
    "pth_num =1\n",
    "noise = True\n",
    "val = 0.01\n",
    "if noise: pth = '.pth'\n",
    "else: pth = '_clean.pth' \n",
    "if sel_model=='pix2pix' or sel_model=='pix2pix2': pth='_g.pth'\n",
    "save_record = False # 기록용 폴더에 있으면\n",
    "pth_dir ='./save/best_'+sel_model+'_model'+str(pth_num)+pth\n",
    "if save_record:\n",
    "    pth_dir ='./기록용/'+sel_model+'/'+'best_'+sel_model+'_model'+str(pth_num)+pth\n",
    "# load_loss_pth ='./ResNet18_model2.pth'\n",
    "\n",
    "display_number = 10                        # plt로 디스플레이할 이미지의 개수\n",
    "residual_output_paths = '/content/residuals'           # 잔차 이미지를 저장할 폴더\n",
    "noisy_image_paths = '/local_datasets/MLinP/train/scan'\n",
    "clean_image_paths = '/local_datasets/MLinP/train/clean'\n",
    "\n",
    "seed_everything(42)\n",
    "output = False                                         # residual을 파일로 출력할지 여부\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def cvt_to_image(image):\n",
    "    image = image.cpu().squeeze(0)\n",
    "     # denoised_image = torch.clamp(denoised_image, 0, 1)  # 이미지 값을 0과 1 사이로 클램핑\n",
    "    image = ToPILImage()(image*0.5+0.5)\n",
    "    image = np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "    image = image[:, :, 0]\n",
    "    return image\n",
    "\n",
    "class CustomDatasetTest(data.Dataset):\n",
    "    def __init__(self, noisy_image_paths, clean_image_paths, transform=None):\n",
    "        self.noisy_image_paths = [join(noisy_image_paths, x) for x in listdir(noisy_image_paths)]\n",
    "        self.clean_image_paths = [join(clean_image_paths, x) for x in listdir(clean_image_paths)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        noisy_image_path = self.noisy_image_paths[index]\n",
    "        noisy_image = load_img(self.noisy_image_paths[index])\n",
    "        clean_image_path = self.clean_image_paths[index]\n",
    "        clean_image = load_img(self.clean_image_paths[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "            clean_image = self.transform(clean_image)\n",
    "\n",
    "        return noisy_image, noisy_image_path, clean_image, clean_image_path\n",
    "\n",
    "# Transform Definition\n",
    "class RGBtoYcrcb(object):\n",
    "    def __init__(self, ):\n",
    "        return \n",
    "\n",
    "    def __call__(self,sample):\n",
    "        image = sample\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_RGB2YCrCb)\n",
    "        return image\n",
    "\n",
    "if sel_model == 'DnCNN':\n",
    "    model = DnCNN.DnCNN()\n",
    "elif sel_model == 'ResNet18':\n",
    "    model = ResNet.ResNet18()\n",
    "elif sel_model == 'RFDN':\n",
    "    model = RFDN.RFDN()\n",
    "elif sel_model == 'DRLN':\n",
    "    model = DRLN.DRLN()\n",
    "elif sel_model == 'pix2pix':\n",
    "    model = pix2pix.Generator()\n",
    "elif sel_model == 'pix2pix2':\n",
    "    model = pix2pix2.GeneratorUNet()\n",
    "    model_D = pix2pix2.Discriminator()\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "model.load_state_dict(torch.load(pth_dir, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "model.to(device)\n",
    "\n",
    "test_transform = Compose([\n",
    "    # BilateralBlur(512),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "def tensor_to_yuv(image):\n",
    "    image = torch.clamp(image, 0, 1)  # 이미지 값을 0과 1 사이로 클램핑\n",
    "    image = ToPILImage()(image)\n",
    "    image = np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "    return\n",
    "\n",
    "def cal_mae_loss(p,t, y=False):\n",
    "    if not y:\n",
    "        p = p[:, :, 0]\n",
    "        t = t[:, :, 0]\n",
    "    return abs(np.mean(p.flatten()) - np.mean(t.flatten()))\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "dataset = CustomDatasetTest(noisy_image_paths, clean_image_paths ,transform=test_transform)\n",
    "# val 분할\n",
    "train_size = int(len(dataset)*(1-val))\n",
    "val_size = int(len(dataset)*(val))\n",
    "train_dataset, val_dataset = random_split(dataset,[train_size,val_size])\n",
    "# 데이터 로더 설정\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "tot_mae = 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for noisy_image, _, clean_image, _  in val_loader:\n",
    "    noisy_image = noisy_image.to(device)\n",
    "    if noise:\n",
    "        pred_noise = model(noisy_image)\n",
    "        denoised_image = noisy_image - pred_noise  \n",
    "    else:\n",
    "        denoised_image = model(noisy_image)\n",
    "        pred_noise = noisy_image - denoised_image \n",
    "    # denoised_image를 CPU로 이동하여 이미지 저장\n",
    "    residual_image = noisy_image.cpu() - clean_image\n",
    "    clean_image = cvt_to_image(clean_image)\n",
    "    denoised_image = cvt_to_image(denoised_image)\n",
    "    noisy_image = cvt_to_image(noisy_image)\n",
    "    residual_image = cvt_to_image(residual_image)\n",
    "    pred_noise = cvt_to_image(pred_noise)\n",
    "    mae_loss = cal_mae_loss(denoised_image,clean_image,True)\n",
    "    tot_mae += mae_loss\n",
    "    print('This MAE: ',mae_loss)\n",
    "    if True:\n",
    "        plt.figure(figsize=(20,13))\n",
    "        plt.subplot(2,3,1)\n",
    "        plt.imshow(noisy_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2,3,2)\n",
    "        plt.imshow(clean_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2,3,3)\n",
    "        plt.imshow(denoised_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2,3,4)\n",
    "        plt.imshow(residual_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2,3,5)\n",
    "        plt.imshow(pred_noise, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "print(len(val_loader),'개의', 'total MAE: ', tot_mae/len(val_loader))\n",
    "# avg_residual = torch.mean(residuals,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This MAE:  199.56296157836914\n",
      "This MAE:  180.14152145385742\n",
      "This MAE:  137.51591873168945\n",
      "This MAE:  218.08250045776367\n",
      "This MAE:  228.17327499389648\n",
      "This MAE:  199.27943801879883\n",
      "This MAE:  238.3889045715332\n",
      "This MAE:  195.14138793945312\n",
      "This MAE:  236.4406623840332\n",
      "This MAE:  199.15451431274414\n",
      "10 개의 total MAE:  203.18810844421387\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "import torchvision.transforms as tf\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import argparse\n",
    "import models.models.DnCNN as DnCNN, models.models.ResNet as ResNet, models.models.RFDN as RFDN\n",
    "import models.models.DRLN as DRLN, models.models.pix2pix as pix2pix, models.models.pix2pix2 as pix2pix2\n",
    "# from models.utils.param import param_check, seed_everything\n",
    "from models.utils.param import seed_everything\n",
    "import models.utils.vgg_loss, models.utils.vgg_perceptual_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_list = ['DnCNN', 'ResNet18', 'RFDN', 'DRLN', 'pix2pix', 'pix2pix2']\n",
    "\n",
    "sel_model = model_list[4]\n",
    "pth_num =2\n",
    "noise = True\n",
    "val = 0.001\n",
    "if noise: pth = '.pth'\n",
    "else: pth = '_clean.pth' \n",
    "if sel_model=='pix2pix' or sel_model=='pix2pix2': pth='_g.pth'\n",
    "save_record = False # 기록용 폴더에 있으면\n",
    "pth_dir ='./save/best_'+sel_model+'_model'+str(pth_num)+pth\n",
    "if save_record:\n",
    "    pth_dir ='./기록용/'+sel_model+'/'+'best_'+sel_model+'_model'+str(pth_num)+pth\n",
    "# load_loss_pth ='./ResNet18_model2.pth'\n",
    "\n",
    "display_number = 10                        # plt로 디스플레이할 이미지의 개수\n",
    "residual_output_paths = '/content/residuals'           # 잔차 이미지를 저장할 폴더\n",
    "noisy_image_paths = '/local_datasets/MLinP/train/scan'\n",
    "clean_image_paths = '/local_datasets/MLinP/train/clean'\n",
    "\n",
    "seed_everything(42)\n",
    "output = False                                         # residual을 파일로 출력할지 여부\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def cvt_to_image(image):\n",
    "    image = image.cpu().squeeze(0)\n",
    "     # denoised_image = torch.clamp(denoised_image, 0, 1)  # 이미지 값을 0과 1 사이로 클램핑\n",
    "    image = ToPILImage()(image*0.5+0.5)\n",
    "    image = np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "    image = image[:, :, 0]\n",
    "    return image\n",
    "\n",
    "def tensor_to_yuv(images):\n",
    "    i =[]\n",
    "    for image in images:\n",
    "        image = torch.clamp(image, 0, 1)  # 이미지 값을 0과 1 사이로 클램핑\n",
    "        image = ToPILImage()(image)\n",
    "        image = np.array(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "        i.append(image)\n",
    "    return i\n",
    "\n",
    "def cal_mae_loss(ps,ts, y=False):\n",
    "    abss = 0.0\n",
    "    for p, t in zip(ps,ts):\n",
    "        if not y:\n",
    "            p = p[:, :, 0]\n",
    "            t = t[:, :, 0]\n",
    "        abss += abs(np.mean(p.flatten()) - np.mean(t.flatten()))\n",
    "    return abss\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "dataset = CustomDatasetTest(noisy_image_paths, clean_image_paths ,transform=test_transform)\n",
    "# val 분할\n",
    "train_size = int(len(dataset)*(1-val))\n",
    "val_size = int(len(dataset)*(val))\n",
    "train_dataset, val_dataset = random_split(dataset,[train_size,val_size])\n",
    "# 데이터 로더 설정\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "tot_mae = 0.0\n",
    "\n",
    "\n",
    "\n",
    "for noisy_image, _, clean_image, _  in val_loader:\n",
    "    noisy_image = noisy_image\n",
    "    if noise:\n",
    "        pred_noise = noisy_image\n",
    "        denoised_image = noisy_image - pred_noise  \n",
    "    else:\n",
    "        denoised_image = noisy_image\n",
    "        pred_noise = noisy_image - denoised_image \n",
    "    # denoised_image를 CPU로 이동하여 이미지 저장\n",
    "    residual_image = noisy_image.cpu() - clean_image\n",
    "\n",
    "    mae_loss = cal_mae_loss(tensor_to_yuv(denoised_image),tensor_to_yuv(clean_image),False)\n",
    "    tot_mae += mae_loss\n",
    "    clean_image = cvt_to_image(clean_image)\n",
    "    denoised_image = cvt_to_image(denoised_image)\n",
    "    noisy_image = cvt_to_image(noisy_image)\n",
    "    residual_image = cvt_to_image(residual_image)\n",
    "    pred_noise = cvt_to_image(pred_noise)\n",
    "    print('This MAE: ',mae_loss)\n",
    "    if False:\n",
    "        plt.figure(figsize=(20,13))\n",
    "        plt.subplot(2,3,1)\n",
    "        plt.imshow(noisy_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2,3,2)\n",
    "        plt.imshow(clean_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2,3,3)\n",
    "        plt.imshow(denoised_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2,3,4)\n",
    "        plt.imshow(residual_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2,3,5)\n",
    "        plt.imshow(pred_noise, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "print(len(val_loader),'개의', 'total MAE: ', tot_mae/len(val_loader))\n",
    "# avg_residual = torch.mean(residuals,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
