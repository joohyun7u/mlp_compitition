{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 16.93 GB, other allocations: 1.19 GB, max allowed: 18.13 GB). Tried to allocate 128.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 126\u001b[0m\n\u001b[1;32m    124\u001b[0m noisy_image \u001b[39m=\u001b[39m noisy_image\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m noise:\n\u001b[0;32m--> 126\u001b[0m     pred_noise \u001b[39m=\u001b[39m model(noisy_image)\n\u001b[1;32m    127\u001b[0m     denoised_image \u001b[39m=\u001b[39m noisy_image \u001b[39m-\u001b[39m pred_noise  \n\u001b[1;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/mlp_compitition/cjh/models/models/DRLN.py:132\u001b[0m, in \u001b[0;36mDRLN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m x = self.head(x)\n\u001b[1;32m    130\u001b[0m c0 = o0 = x\n\u001b[0;32m--> 132\u001b[0m # b1 = self.b1(o0)\n\u001b[1;32m    133\u001b[0m # c1 = torch.cat([c0, b1], dim=1)\n\u001b[1;32m    134\u001b[0m # o1 = self.c1(c1)\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m # b2 = self.b2(o1)\n\u001b[1;32m    137\u001b[0m # c2 = torch.cat([c1, b2], dim=1)\n\u001b[1;32m    138\u001b[0m # o2 = self.c2(c2)\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m # b3 = self.b3(o2)\n\u001b[1;32m    141\u001b[0m # c3 = torch.cat([c2, b3], dim=1)\n\u001b[1;32m    142\u001b[0m # o3 = self.c3(c3)\n\u001b[1;32m    143\u001b[0m # a1 = o3 + c0\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m # b4 = self.b4(a1)\n\u001b[1;32m    146\u001b[0m # c4 = torch.cat([o3, b4], dim=1)\n\u001b[1;32m    147\u001b[0m # o4 = self.c4(c4)\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m # b5 = self.b5(a1)\n\u001b[1;32m    150\u001b[0m # c5 = torch.cat([c4, b5], dim=1)\n\u001b[1;32m    151\u001b[0m # o5 = self.c5(c5)\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m # b6 = self.b6(o5)\n\u001b[1;32m    154\u001b[0m # c6 = torch.cat([c5, b6], dim=1)\n\u001b[1;32m    155\u001b[0m # o6 = self.c6(c6)\n\u001b[1;32m    156\u001b[0m # a2 = o6 + a1\n\u001b[1;32m    158\u001b[0m b7 = self.b7(o0)\n\u001b[1;32m    159\u001b[0m c7 = torch.cat([c0, b7], dim=1)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/mlp_compitition/cjh/models/models/DRLN.py:50\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m r1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr1(c0)\n\u001b[1;32m     48\u001b[0m c1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([c0, r1], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m r2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mr2(c1)\n\u001b[1;32m     51\u001b[0m c2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([c1, r2], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     53\u001b[0m r3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr3(c2)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/mlp_compitition/cjh/models/models/ops.py:115\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 115\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(x)\n\u001b[1;32m    116\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(out \u001b[39m+\u001b[39m x)\n\u001b[1;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 16.93 GB, other allocations: 1.19 GB, max allowed: 18.13 GB). Tried to allocate 128.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import argparse\n",
    "import models.models.DnCNN as DnCNN, models.models.ResNet as ResNet, models.models.RFDN as RFDN, models.models.DRLN as DRLN\n",
    "# from models.utils.param import param_check, seed_everything\n",
    "import models.utils.vgg_loss, models.utils.vgg_perceptual_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_list = ['DnCNN', 'ResNet18', 'RFDN', 'DRLN']\n",
    "\n",
    "sel_model = model_list[3]\n",
    "pth_num =3\n",
    "noise = True\n",
    "if noise: pth = '.pth'\n",
    "else: pth = '_clean.pth' \n",
    "pth_dir ='./save/best_'+sel_model+'_model'+str(pth_num)+pth\n",
    "# load_loss_pth ='./ResNet18_model2.pth'\n",
    "\n",
    "display_number = 3                            # plt로 디스플레이할 이미지의 개수\n",
    "residual_output_paths = '/content/residuals'           # 잔차 이미지를 저장할 폴더\n",
    "noisy_image_paths = '/local_datasets/MLinP/train/scan'\n",
    "clean_image_paths = '/local_datasets/MLinP/train/clean'\n",
    "\n",
    "random.seed(3002)\n",
    "output = False                                         # residual을 파일로 출력할지 여부\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def cvt_to_image(image):\n",
    "    image = image.cpu().squeeze(0)\n",
    "     # denoised_image = torch.clamp(denoised_image, 0, 1)  # 이미지 값을 0과 1 사이로 클램핑\n",
    "    image = ToPILImage()(image*0.5+0.5)\n",
    "    return image\n",
    "\n",
    "class CustomDatasetTest(data.Dataset):\n",
    "    def __init__(self, noisy_image_paths, transform=None):\n",
    "        self.noisy_image_paths = [join(noisy_image_paths, x) for x in listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        noisy_image_path = self.noisy_image_paths[index]\n",
    "        noisy_image = load_img(self.noisy_image_paths[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "\n",
    "        return noisy_image, noisy_image_path\n",
    "\n",
    "class BilateralBlur(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, int)\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self,sample):\n",
    "        image = sample\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        return cv2.bilateralFilter(image,-1,10,5)\n",
    "\n",
    "if sel_model == 'DnCNN':\n",
    "    model = DnCNN.DnCNN()\n",
    "elif sel_model == 'ResNet18':\n",
    "    model = ResNet.ResNet18()\n",
    "elif sel_model == 'RFDN':\n",
    "    model = RFDN.RFDN()\n",
    "elif sel_model == 'DRLN':\n",
    "    model = DRLN.DRLN()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "model.load_state_dict(torch.load(pth_dir, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "model.to(device)\n",
    "\n",
    "test_transform = Compose([\n",
    "    BilateralBlur(512),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "noisy_dataset = CustomDatasetTest(noisy_image_paths, transform=test_transform)\n",
    "clean_dataset = CustomDatasetTest(clean_image_paths, transform=test_transform)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "# noisy_loader = DataLoader(noisy_dataset, batch_size=1, shuffle=False)\n",
    "# clean_loader = DataLoader(clean_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "random_idx = random.sample(range(len(noisy_dataset)), display_number)\n",
    "\n",
    "# residuals = torch.Tensor(51, 512, 512, 3)\n",
    "# for i, ((noisy_image, noisy_image_path), (clean_image, clean_image_path) ) in enumerate(zip(noisy_loader,clean_loader)):\n",
    "#     a=1\n",
    "for idx in random_idx:\n",
    "    noisy_image, _ = noisy_dataset.__getitem__(idx)\n",
    "    clean_image, _ = clean_dataset.__getitem__(idx)\n",
    "    # residuals[i] = residual_image\n",
    "    noisy_image = noisy_image.unsqueeze(0).to(device)\n",
    "    if noise:\n",
    "        pred_noise = model(noisy_image)\n",
    "        denoised_image = noisy_image - pred_noise  \n",
    "    else:\n",
    "        denoised_image = model(noisy_image)\n",
    "        pred_noise = noisy_image - denoised_image \n",
    "    # denoised_image를 CPU로 이동하여 이미지 저장\n",
    "    residual_image = noisy_image.cpu() - clean_image.unsqueeze(0)\n",
    "\n",
    "    denoised_image = cvt_to_image(denoised_image)\n",
    "    noisy_image = cvt_to_image(noisy_image)\n",
    "    clean_image = cvt_to_image(clean_image)\n",
    "    residual_image = cvt_to_image(residual_image)\n",
    "    pred_noise = cvt_to_image(pred_noise)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20,13))\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.imshow(noisy_image)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,2)\n",
    "    plt.imshow(clean_image)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,3)\n",
    "    plt.imshow(denoised_image)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,4)\n",
    "    plt.imshow(residual_image)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,5)\n",
    "    plt.imshow(pred_noise)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# avg_residual = torch.mean(residuals,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
